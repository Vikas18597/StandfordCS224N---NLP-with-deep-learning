{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5gBQ4/yskmvdrcti7f/AH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0J4ZFufvWOs","executionInfo":{"status":"ok","timestamp":1682180035740,"user_tz":-330,"elapsed":5591,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}},"outputId":"09407772-69a5-4944-d2ce-2fbc82f1b177"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n"]}]},{"cell_type":"code","source":["import math\n","from typing import List\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import nltk\n","import sentencepiece as spm\n","nltk.download('punkt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSVmPjcavKv_","executionInfo":{"status":"ok","timestamp":1682180041431,"user_tz":-330,"elapsed":932,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}},"outputId":"e9281cbc-b868-45e9-c255-3fa3955ad638"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":37,"metadata":{"id":"yJxsX2a-vCs1","executionInfo":{"status":"ok","timestamp":1682184414395,"user_tz":-330,"elapsed":1213,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}}},"outputs":[],"source":["def pad_sents(sents, pad_token):\n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","        The paddings should be at the end of each sentence.\n","    @param sents (list[list[str]]): list of sentences, where each sentence\n","                                    is represented as a list of words\n","    @param pad_token (str): padding token\n","    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentences in the batch now has equal length.\n","    \"\"\"\n","    sents_padded = []\n","\n","    ### YOUR CODE HERE (~6 Lines)\n","    max_sent = max([len(i) for i in sents])\n","    for i in range(len(sents)):\n","      temp = [sents[i][j] if j < len(sents[i]) else pad_token for j in range(max_sent)]\n","      sents_padded.append(temp)\n","\n","    ### END YOUR CODE\n","\n","    return sents_padded"]},{"cell_type":"code","source":["def read_corpus(file_path, source, vocab_size=2500):\n","    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n","    @param file_path (str): path to file containing corpus\n","    @param source (str): \"tgt\" or \"src\" indicating whether text\n","        is of the source language or target language\n","    @param vocab_size (int): number of unique subwords in\n","        vocabulary when reading and tokenizing\n","    \"\"\"\n","    data = []\n","    sp = spm.SentencePieceProcessor()\n","    sp.load('{}.model'.format(source))\n","\n","    with open(file_path, 'r', encoding='utf8') as f:\n","        for line in f:\n","            subword_tokens = sp.encode_as_pieces(line)\n","            # only append <s> and </s> to the target sentence\n","            if source == 'tgt':\n","                subword_tokens = ['<s>'] + subword_tokens + ['</s>']\n","            data.append(subword_tokens)\n","\n","    return data\n","\n","\n","def autograder_read_corpus(file_path, source):\n","    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n","    @param file_path (str): path to file containing corpus\n","    @param source (str): \"tgt\" or \"src\" indicating whether text\n","        is of the source language or target language\n","    \"\"\"\n","    data = []\n","    for line in open(file_path):\n","        sent = nltk.word_tokenize(line)\n","        # only append <s> and </s> to the target sentence\n","        if source == 'tgt':\n","            sent = ['<s>'] + sent + ['</s>']\n","        data.append(sent)\n","\n","    return data\n","\n","\n","def batch_iter(data, batch_size, shuffle=False):\n","    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n","    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (int): batch size\n","    @param shuffle (boolean): whether to randomly shuffle the dataset\n","    \"\"\"\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        examples = [data[idx] for idx in indices]\n","\n","        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n","        src_sents = [e[0] for e in examples]\n","        tgt_sents = [e[1] for e in examples]\n","\n","        yield src_sents, tgt_sents"],"metadata":{"id":"mdu9teDQyoMl","executionInfo":{"status":"ok","timestamp":1682184484424,"user_tz":-330,"elapsed":415,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}}},"execution_count":41,"outputs":[]}]}