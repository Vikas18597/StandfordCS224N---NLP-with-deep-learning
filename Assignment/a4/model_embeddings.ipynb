{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwVWoZmrwBcW1bXo9TRtVZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"4P-JNVMifLjZ","executionInfo":{"status":"ok","timestamp":1682715706954,"user_tz":-330,"elapsed":4277,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"K1Rar7-n-8LA","executionInfo":{"status":"ok","timestamp":1682715710692,"user_tz":-330,"elapsed":22,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}}},"outputs":[],"source":["class ModelEmbeddings(nn.Module): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, embed_size, vocab):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # default values\n","        self.source = None\n","        self.target = None\n","\n","        src_pad_token_idx = vocab.src['<pad>']\n","        tgt_pad_token_idx = vocab.tgt['<pad>']\n","\n","        ### YOUR CODE HERE (~2 Lines)\n","        ### TODO - Initialize the following variables:\n","        ###     self.source (Embedding Layer for source language)\n","        ###     self.target (Embedding Layer for target langauge)\n","        ###\n","        ### Note:\n","        ###     1. `vocab` object contains two vocabularies:\n","        ###            `vocab.src` for source\n","        ###            `vocab.tgt` for target\n","        ###     2. You can get the length of a specific vocabulary by running:\n","        ###             `len(vocab.<specific_vocabulary>)`\n","        ###     3. Remember to include the padding token for the specific vocabulary\n","        ###        when creating your Embedding.\n","        ###\n","        ### Use the following docs to properly initialize these variables:\n","        ###     Embedding Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n","        self.source = nn.Embedding(len(vocab.src),embed_size,padding_idx=src_pad_token_idx)\n","        self.target = nn.Embedding(len(vocab.tgt),embed_size,padding_idx=tgt_pad_token_idx)\n","        \n","\n","\n","        ### END YOUR CODE"]}]}