{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdYyL+ga9Jy3eaB91ekeJZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install sentencepiece\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcOBSwStm7J8","executionInfo":{"status":"ok","timestamp":1682714763578,"user_tz":-330,"elapsed":5434,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}},"outputId":"74b1dddc-9d91-44c8-8222-b9c422c4a0ee"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133},"id":"V3WajNgKmpwC","executionInfo":{"status":"error","timestamp":1682714810828,"user_tz":-330,"elapsed":531,"user":{"displayName":"Vikas Khati","userId":"10093839171750109065"}},"outputId":"e03138e0-fd78-4f96-b909-2ddd98c9a2ae"},"outputs":[{"output_type":"error","ename":"DocoptExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mDocoptExit\u001b[0m\u001b[0;31m:\u001b[0m Usage:\n    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n"]}],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","from collections import Counter\n","from docopt import docopt\n","from itertools import chain\n","import json\n","import torch\n","from typing import List\n","# from utils import read_corpus, pad_sents\n","import sentencepiece as spm\n","\n","\n","class VocabEntry(object):\n","    \"\"\" Vocabulary Entry, i.e. structure containing either\n","    src or tgt language terms.\n","    \"\"\"\n","    def __init__(self, word2id=None):\n","        \"\"\" Init VocabEntry Instance.\n","        @param word2id (dict): dictionary mapping words 2 indices\n","        \"\"\"\n","        if word2id:\n","            self.word2id = word2id\n","        else:\n","            self.word2id = dict()\n","            self.word2id['<pad>'] = 0   # Pad Token\n","            self.word2id['<s>'] = 1 # Start Token\n","            self.word2id['</s>'] = 2    # End Token\n","            self.word2id['<unk>'] = 3   # Unknown Token\n","        self.unk_id = self.word2id['<unk>']\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        \"\"\" Retrieve word's index. Return the index for the unk\n","        token if the word is out of vocabulary.\n","        @param word (str): word to look up.\n","        @returns index (int): index of word \n","        \"\"\"\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        \"\"\" Check if word is captured by VocabEntry.\n","        @param word (str): word to look up\n","        @returns contains (bool): whether word is contained    \n","        \"\"\"\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        \"\"\" Raise error, if one tries to edit the VocabEntry.\n","        \"\"\"\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        \"\"\" Compute number of words in VocabEntry.\n","        @returns len (int): number of words in VocabEntry\n","        \"\"\"\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        \"\"\" Representation of VocabEntry to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        \"\"\" Return mapping of index to word.\n","        @param wid (int): word index\n","        @returns word (str): word corresponding to index\n","        \"\"\"\n","        return self.id2word[wid]\n","\n","    def add(self, word):\n","        \"\"\" Add word to VocabEntry, if it is previously unseen.\n","        @param word (str): word to add to VocabEntry\n","        @return index (int): index that the word has been assigned\n","        \"\"\"\n","        if word not in self:\n","            wid = self.word2id[word] = len(self)\n","            self.id2word[wid] = word\n","            return wid\n","        else:\n","            return self[word]\n","\n","    def words2indices(self, sents):\n","        \"\"\" Convert list of words or list of sentences of words\n","        into list or list of list of indices.\n","        @param sents (list[str] or list[list[str]]): sentence(s) in words\n","        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n","        \"\"\"\n","        if type(sents[0]) == list:\n","            return [[self[w] for w in s] for s in sents]\n","        else:\n","            return [self[w] for w in sents]\n","\n","    def indices2words(self, word_ids):\n","        \"\"\" Convert list of indices into words.\n","        @param word_ids (list[int]): list of word ids\n","        @return sents (list[str]): list of words\n","        \"\"\"\n","        return [self.id2word[w_id] for w_id in word_ids]\n","\n","    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n","        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n","        shorter sentences.\n","\n","        @param sents (List[List[str]]): list of sentences (words)\n","        @param device: device on which to load the tesnor, i.e. CPU or GPU\n","\n","        @returns sents_var: tensor of (max_sentence_length, batch_size)\n","        \"\"\"\n","        word_ids = self.words2indices(sents)\n","        sents_t = pad_sents(word_ids, self['<pad>'])\n","        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n","        return torch.t(sents_var)\n","\n","    @staticmethod\n","    def from_corpus(corpus, size, freq_cutoff=2):\n","        \"\"\" Given a corpus construct a Vocab Entry.\n","        @param corpus (list[str]): corpus of text produced by read_corpus function\n","        @param size (int): # of words in vocabulary\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n","        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n","        \"\"\"\n","        vocab_entry = VocabEntry()\n","        word_freq = Counter(chain(*corpus))\n","        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n","        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n","              .format(len(word_freq), freq_cutoff, len(valid_words)))\n","        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n","        for word in top_k_words:\n","            vocab_entry.add(word)\n","        return vocab_entry\n","    \n","    @staticmethod\n","    def from_subword_list(subword_list):\n","        vocab_entry = VocabEntry()\n","        for subword in subword_list:\n","            vocab_entry.add(subword)\n","        return vocab_entry\n","\n","\n","class Vocab(object):\n","    \"\"\" Vocab encapsulating src and target langauges.\n","    \"\"\"\n","    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n","        \"\"\" Init Vocab.\n","        @param src_vocab (VocabEntry): VocabEntry for source language\n","        @param tgt_vocab (VocabEntry): VocabEntry for target language\n","        \"\"\"\n","        self.src = src_vocab\n","        self.tgt = tgt_vocab\n","\n","    @staticmethod\n","    def build(src_sents, tgt_sents) -> 'Vocab':\n","        \"\"\" Build Vocabulary.\n","        @param src_sents (list[str]): Source subwords provided by SentencePiece\n","        @param tgt_sents (list[str]): Target subwords provided by SentencePiece\n","        \"\"\"\n","\n","        print('initialize source vocabulary ..')\n","        src = VocabEntry.from_subword_list(src_sents)\n","\n","        print('initialize target vocabulary ..')\n","        tgt = VocabEntry.from_subword_list(tgt_sents)\n","\n","        return Vocab(src, tgt)\n","\n","    def save(self, file_path):\n","        \"\"\" Save Vocab to file as JSON dump.\n","        @param file_path (str): file path to vocab file\n","        \"\"\"\n","        with open(file_path, 'w') as f:\n","            json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), f, indent=2)\n","\n","    @staticmethod\n","    def load(file_path):\n","        \"\"\" Load vocabulary from JSON dump.\n","        @param file_path (str): file path to vocab file\n","        @returns Vocab object loaded from JSON dump\n","        \"\"\"\n","        entry = json.load(open(file_path, 'r'))\n","        src_word2id = entry['src_word2id']\n","        tgt_word2id = entry['tgt_word2id']\n","\n","        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n","\n","    def __repr__(self):\n","        \"\"\" Representation of Vocab to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n","\n","\n","def get_vocab_list(file_path, source, vocab_size):\n","    \"\"\" Use SentencePiece to tokenize and acquire list of unique subwords.\n","    @param file_path (str): file path to corpus\n","    @param source (str): tgt or src\n","    @param vocab_size: desired vocabulary size\n","    \"\"\" \n","    spm.SentencePieceTrainer.train(input=file_path, model_prefix=source, vocab_size=vocab_size)     # train the spm model\n","    sp = spm.SentencePieceProcessor()                                                               # create an instance; this saves .model and .vocab files \n","    sp.load('{}.model'.format(source))                                                              # loads tgt.model or src.model\n","    sp_list = [sp.id_to_piece(piece_id) for piece_id in range(sp.get_piece_size())]                 # this is the list of subwords\n","    return sp_list \n","\n","\n","\n","if __name__ == '__main__':\n","    args = docopt(__doc__)\n","\n","    print('read in source sentences: %s' % args['--train-src'])\n","    print('read in target sentences: %s' % args['--train-tgt'])\n","\n","    src_sents = get_vocab_list(args['--train-src'], source='src', vocab_size=21000)         \n","    tgt_sents = get_vocab_list(args['--train-tgt'], source='tgt', vocab_size=8000)\n","    vocab = Vocab.build(src_sents, tgt_sents)\n","    print('generated vocabulary, source %d words, target %d words' % (len(src_sents), len(tgt_sents)))\n","\n","    vocab.save(args['VOCAB_FILE'])\n","    print('vocabulary saved to %s' % args['VOCAB_FILE'])\n"]}]}